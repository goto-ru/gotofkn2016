{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (Natural Language  - библиотека для работы с текстами на языке python. Книга по NLTK есть по ссылке http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Токенизация</h2>\n",
    "<strong>Токенизация</strong> - разбивка текста на <strong>токены</strong>. Токен чаще всего соответствует слову(но не всегда)\n",
    "В nltk за токенизацию отвечает модуль nltk.tokenzie содержащий различные методы разбивки на токены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простая токенизация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама\n",
      "мыла\n",
      "раму\n",
      "--\n",
      "Ростов-на-дону\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "for token in wordpunct_tokenize(\"мама мыла раму\"):\n",
    "    print(token)\n",
    "print('--')\n",
    "for token in wordpunct_tokenize(\"Ростов-на-дону\"):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация по регулярному выражению:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клара\n",
      "у\n",
      "карла\n",
      "украла\n",
      "кораллы\n",
      "клара\n",
      "у\n",
      "карла\n",
      "украла\n",
      "кларнет\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(ur\"[^ ,\\.\\!\\?\\:]+\")\n",
    "for token in tokenizer.tokenize(u\"Клара у карла украла кораллы, клара у карла украла кларнет\"):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Лемматизация и стемминг</h2> \n",
    "<strong>Лемматизация</strong> - приведение слова к \"нормальной\" форме(\"мыла\" -> \"мыть\"). \n",
    "Не всегда однозначно: \"Вина\" -> \"Вина\" или \"Вино\"\n",
    "Для большинства практических применений подходит <strong>\"стемминг\"</strong> - процесс нахождения основы слова и отброса окончания. В nltk находится nltk.stem\n",
    "Стеммер русского языка находится в модуле <strong>nltk.stem.snowball</strong> и называется <strong>RussianStemmer</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мам\n",
      "мыл\n",
      "рам\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "stemmer = RussianStemmer()\n",
    "for token in tokenizer.tokenize(u\"Мама мыла раму\"):\n",
    "    print (stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Частотный анализ</h2>\n",
    "<h3>Мешок слов</h3>\n",
    "Компьютеру очень сложно работать с текстами - понимать смысл, взаимосвязи между словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "у\t2\n",
      "клар\t2\n",
      "карл\t2\n",
      "укра\t2\n",
      "коралл\t1\n",
      "кларнет\t1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def get_bag_of_the_words(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    return Counter(tokens_stemmed)\n",
    "freq = get_bag_of_the_words(u\"карл у клары украл кораллы, клара у карла украла кларнет\")\n",
    "for token, freq in freq.most_common():\n",
    "    print (token + \"\\t\" + str(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Задание</h3>\n",
    "Рассчитайте \"Мешок слов\" для стихотворения \"бородино\" Лермонтова. Все слова необходимо токенизировать и лемматизировать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>О чем текст?</h2>\n",
    "<img src=\"https://i.gyazo.com/956df0997a8f6219df3615a330df2157.png\" width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TfIdf (term frequency-inverted document frequency)</h3>\n",
    "Так как при преобразовании \"мешок слов\" большой вес имеют предлоги,\n",
    "союзы и другие слова несущие мало смысловой нагрузки - необходимо уменьшить их вес. \n",
    "Это делается при помощи домножения на специальный коэффицент <strong>IDF</strong>\n",
    "(логарифм обратной частоты встречаемости)\n",
    "\n",
    "https://ru.wikipedia.org/wiki/TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class TfIdfTransformer(object):\n",
    "    def __init__(self, stemmer, tokenizer):\n",
    "        self.stemmer = stemmer \n",
    "        self.tokenizer = tokenizer\n",
    "        self.idf = {}\n",
    "    def fit(self, texts):\n",
    "        token_freq = Counter()\n",
    "        for text in texts:\n",
    "            doc_tokens = set([self.stemmer.stem(token) for token in self.tokenizer.tokenize(text)])\n",
    "            for token in doc_tokens:\n",
    "                token_freq[token] += 1\n",
    "        self.idf = {}\n",
    "        for token in token_freq:\n",
    "            self.idf[token] = math.log(len(texts)/token_freq[token])\n",
    "        \n",
    "    def transform(self, text):\n",
    "        text_tokens = [self.stemmer.stem(token) for token in self.tokenizer.tokenize(text)]\n",
    "        doc_tokens = Counter(text_tokens)\n",
    "        result = {}\n",
    "        for token in doc_tokens:\n",
    "            result[token] = self.idf.get(token, 0)  * doc_tokens[token]/len(text_tokens)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример:\n",
    "обучим на сете твитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "mongo = pymongo.MongoClient('goto.reproducible.work')\n",
    "db = mongo.twits\n",
    "texts_positive = [doc['text'] for doc in db.posts.find({'positive': 1})[:1000]]\n",
    "texts_negative = [doc['text'] for doc in db.posts.find({'positive': 0})[:1000]]\n",
    "texts_all = texts_positive + texts_negative\n",
    "\n",
    "tf_idf_transformer = TfIdfTransformer(stemmer, tokenizer)\n",
    "tf_idf_transformer.fit(texts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "как\t0.69314718056\n",
      "неб\t2.30258509299\n",
      "голуб\t2.53363415318\n"
     ]
    }
   ],
   "source": [
    "transformed = tf_idf_transformer.transform(u\"Какое небо голубое!\")\n",
    "for token in transformed.keys():\n",
    "    print (token + \"\\t\" + str(transformed[token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Определение принадлежности текста к группе(классификация)</h3>\n",
    "TfIdf можно использовать для определения наиболее характерных токенов в группе текстов. Для этого сначала необходимо найти \"средние\" значения tf-idf в группе, а затем посчитать насколько близки значения средние значения tf-idf в группе и у заданного текста. \n",
    "В качестве меры близости обычно используется <a href=\"https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C#.D0.9A.D0.BE.D1.81.D0.B8.D0.BD.D1.83.D1.81.D0.BD.D0.BE.D0.B5_.D1.81.D1.85.D0.BE.D0.B4.D1.81.D1.82.D0.B2.D0.BE\">косинусная мера</a>: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#функция нормировки. нужна для корректного подсчета косинусной меры\n",
    "def norm(vec):\n",
    "    s = 0.0\n",
    "    for item in vec:\n",
    "        s += vec[item] ** 2\n",
    "    s = math.sqrt(s)\n",
    "    result = {}\n",
    "    for item in vec:\n",
    "        result[item] = vec[item]/s\n",
    "    return result\n",
    "\n",
    "#функция для рассчета среднего значения tf-idf в группе текстов\n",
    "def get_average_tf_idf(texts):\n",
    "    result = defaultdict(lambda: 0)\n",
    "    for text in texts:\n",
    "        transformed = tf_idf_transformer.transform(text)\n",
    "        for token in transformed:\n",
    "            result[token] += transformed[token]\n",
    "    return norm(result)\n",
    "\n",
    "\n",
    "#функция для рассчета близости между двумя текстом и средним значением tf-idf для группы\n",
    "def get_sim(text, tf_idf_group):\n",
    "    tf_idf = norm(tf_idf_transformer.transform(text))\n",
    "    result = 0\n",
    "    for token in tf_idf:\n",
    "        result += tf_idf[token] * tf_idf_group.get(token, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_tf_idf_positive = get_average_tf_idf(texts_positive)\n",
    "average_tf_idf_negative = get_average_tf_idf(texts_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('positive', 0.08587693929834633)\n",
      "('negative', 0.05415204617630549)\n"
     ]
    }
   ],
   "source": [
    "text = u'я рад,ведь сегодня впереди вся ночь, идеальная ночка:)'\n",
    "print (\"positive\", get_sim(text, average_tf_idf_positive))\n",
    "print (\"negative\", get_sim(text, average_tf_idf_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
